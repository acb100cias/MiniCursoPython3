{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas en complejidad computacional y eficiencia algorítmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTA**: lo que vamos hacer y a ver continuación es calcular o acotar la complejidad de algoritmos explícitamente dados, a esto se le llama _análsis de algortimos_. No debemos confundir esto con la _Teoría de Complejidad Computacional_ ya que en esta se estudia la _complejidad de los problemas_, se responde a preguntas como qué tan complejo es el ajedrez, el Go, el Monopoly o el problema del hombre de negocios. Acá también se estudia el problema P vs NP, y bueno, esto es lo que no son estas notas.  \n",
    "¡Pero las notas tienen en el titúlo _complejidad computacional_! Lo sé y por esa va esta advertencia. Con estas notas no será posible entender cómo se realizan los calculos de la complejidad de los problemas pero sí podremos calcular el complejidad para un algoritmo dado en términos de tiempo y memoria.  \n",
    "Lo sé, ¡vaya lío!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando hablemos de la _complejidad computacional_ o simplemente _complejidad_ de un algoritmo nos vamos a referir a la cantidad de recursos requeridos para poder correlo, ya sea en memoria o en tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **complejidad temporal** (*time complexity*) está relacionada a la cantidad de operaciones elementales que se deben de realizar en cierto algortimo para una entrada (input) de tamaño $n$ mientras que la **complejidad espacial** (*space complexity*) está relacionada con la cantidad de memoria requerida por un algortimo con un input de tamaño $n$.  \n",
    "Notemos que la complejidad temporal no tiene unidades ni en segundos ni en minutos sino en operaciones elementales. Cada computador tiene un componentes distintos por lo que convertir esta cantidad al tiempo en que cada computador puede realizar esas operaciones de acuerdo al hardware hace más útil esta medida que tener que estar configurando esta función con parámetros especiales a cada computador. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el análisis que se realizará en torno a los algortimos asume que estaremos trabajando con computadoras _Modelo RAM_ (las computadoras actuales) donde cada unidad de memoria puede representar un número real y acceder a una locación de memoria siempre toma tiempo constante sin importar dónde esté localizada. De igual manera vamos estar asumiendo que la computadora puede lidiar con representaciones de números reales de precisión arbitratia con el mismo costo. Las operaciones elementales serán:\n",
    "1. Obtener la parte entera de un número real\n",
    "2. Comparar dos números\n",
    "3. Todas las cuatro operaciones aritméticas básicas\n",
    "4. Todas las funciones matemáticas básicas ($log_n(x)$, trigonométricas, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada problema estará especificado por un conjunto de datos llamados input. El tamaño del input es el número de unidades de memoria necesarias para poder representar dicho input. Cuando todos los elementos del input son elementales entonces el tamaño del input es proporcional al número de elementos del input. \n",
    "\n",
    "Ahora bien, cuando un algoritmo se ejecuta es natural esperar que cierta cantidad de operaciones elementales se realicen en función del tamaño del input, el tiempo de ejecución depende del tamaño del input.\n",
    "La _complejidad en el peor de los casos_ es una función $f(n)$ o la cota superior del número de operaciones elementales que va a ejecutar el algoritmo cuando el tamaño del inputn es $n$.\n",
    "\n",
    "La complejidad en el peor escenario es una estimación pesimista del tiempo de ejecución de un algoritmo y en la realidad esta cota superior es alcanzada muy raras ocasiones y realizando un preprocesado de los datos es totalmente evitable. Por lo anterior es que la _complejidad en el caso promedio_ es un mejor estimador. La complejidad en el caso promedio es la función $g(n)$ que da el número promedio de operaciones (en un sentido estadístico) que se van e ejecutar si se proporciona una medida de probabilidad sobre todos los inputs de tamaño $n$. Sin embargo es mucho más difícil calcular la probabilidad en el caso promedio que en peor de los casos ya que muy rara vez es posible construir una medidad de probabilidad sobre el espacio de todos los inputs de tamaño $n$ que realmente represente a los posibles inputs de tamaño $n$ para un algoritmo dado.\n",
    "\n",
    "La complejidad espacial en el peor de los casos y la complejidad espacial en el caso promedio se define de igual manera que lo anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A lo largo de este notebook vamos a trabajar con la complejidad en el peor de los escenarios y al referirnos a la complejidad del algoritmo A nos vamos a referir a la complejidad en el peor de los casos. De igual manera con la complejidad espacial.  \n",
    "Notemos que referirnos a _el algortimo A se ejecuta en tiempo $f(n)$_ es equivalente a decir _la complejidad del algoritmo A es $f(n)$_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
